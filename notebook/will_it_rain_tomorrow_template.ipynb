{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the Code Pub!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is called a jupyter notebook. It is a web-based interactive computational environment for creating notebook documents in which you can write code in separate cells and run it. It is commonly used amongst Data Scientists for experimenting with data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try hitting shift+enter in the cell below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After each cell has run it does not have to be run again. All variables are saved in the scope of the notebook. Lets import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some information about the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we are going to work with is from the dataset \"Rain in Australia\" from kaggle.com. https://www.kaggle.com/jsphyg/weather-dataset-rattle-package . The use case is to predict if it will rain tomorrow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first step is to load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We use pandas to load the data \n",
    "df = pd.read_csv('WeatherAUS.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next step is to explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by printing the data and look at the columns\n",
    "\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's have a look at the data types of the columns \n",
    "\n",
    "#df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets have a look at the columns that are \"objects\"\n",
    "\n",
    "#df[['Date', 'Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday', 'RainTomorrow']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the non-numeric values it is necessary to see the number of unique values. \n",
    "# We will have to transform these columns to numeric values later. \n",
    "# Therefore we would like to know how many unique values they have. \n",
    "\n",
    "#object_column_names = ['Date', 'Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday', 'RainTomorrow']\n",
    "#for column in object_column_names:\n",
    "#    print(column + ': ' + str(len(df[column].unique())) + \" unique values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also see some columns with NaN values. Let's see how many rows per column are not NaN\n",
    "\n",
    "#df.count().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's use pandas describe() function to explore the data some more\n",
    "\n",
    "#df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# During the exploration we saw 4 columns that have more than 40% NaN values. \n",
    "# We will start by dropping these columns because we are not sure how to replace the empty values. \n",
    "\n",
    "#df = df.drop(columns=['Sunshine', 'Evaporation', 'Cloud3pm', 'Cloud9am'])\n",
    "#df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are different eays of dealing with empty values. \n",
    "# But we don't have enough time during this workshop to explore those options.\n",
    "# So we will drop all rows that have any NaN values. \n",
    "\n",
    "#df = df.dropna(how='any')\n",
    "#df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The goal of this use case is to see how well we can predict if it will rain tomorrow in a given city.\n",
    "# This is a boolean classification problem. \n",
    "# So we will begin with mapping the two binary columns \"RainToday\" and \"RainTomorrow\" to 0/1\n",
    "\n",
    "#df['RainTomorrow'] = df['RainTomorrow'].map({'Yes':1, 'No':0})\n",
    "#df['RainToday'] = df['RainToday'].map({'Yes':1, 'No':0})\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will drop the date column because we will assume that seasonal information can already be reflected in the meteorological data\n",
    "\n",
    "#df = df.drop(columns=['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will also drop the RISK_MM column because it leaks information according to the description\n",
    "\n",
    "#df = df.drop(columns=['RISK_MM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the categorical columns we will perform one hot encoding. \n",
    "# We will do this by using pandas get_dummies() function.\n",
    "# Since the columns didn't have a large range of unique values, one hot encoding is a good option.\n",
    "\n",
    "#df = pd.get_dummies(df)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will look at the correlation between \"RainTomorrow\" and the other columns\n",
    "\n",
    "#df.corr().loc[['RainTomorrow']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data and normalize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will seperate the training columns and the target column\n",
    "\n",
    "#X = df.drop(columns=['RainTomorrow'])\n",
    "#y = df['RainTomorrow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will split our data into test and training datasets using train_test_split from sklearn\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the MinMax scaler to normalize the data \n",
    "\n",
    "#scaler = MinMaxScaler()\n",
    "#columns = X.columns\n",
    "#X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train))\n",
    "#X_train_scaled.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the scaler is fit, we will transform the test data with the scaler\n",
    "\n",
    "#X_test_scaled = pd.DataFrame(scaler.transform(X_test))\n",
    "#X_test_scaled.columns = columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will set the class_weight to balanced because the classes are not balanced.\n",
    "# model info: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "#model = DecisionTreeClassifier(max_depth=3, class_weight=\"balanced\")\n",
    "#model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The score is the number of data points that were correctly classified\n",
    "\n",
    "#model.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is also important to look at the confusion matrix to see the number of correctly classified data points\n",
    "# in each class.\n",
    "\n",
    "# Let's have a look at the confusion matrix \n",
    "\n",
    "#predicted = clf.predict(X_test_scaled)\n",
    "#true_neg, false_pos, false_neg, true_pos = confusion_matrix(y_test, predicted).ravel()\n",
    "#true_neg, false_pos, false_neg, true_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets plot the tree\n",
    "# Which features are at the top layers? \n",
    "\n",
    "#from sklearn import tree\n",
    "#plot = tree.plot_tree(clf, fontsize=10, filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now you can experiment with other models :)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are three example models to experiment with\n",
    "# You can try and experiment with different hyperparameters\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(class_weight=\"balanced\"), \n",
    "    \"RandomForestClassifier\": RandomForestClassifier(), \n",
    "    \"GradientBoostingClassifier\": GradientBoostingClassifier()\n",
    "    # ...\n",
    "    # Add more models ?! See scikit-learn documentation\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score each model\n",
    "# Would it also be interesting to print the confusion matrix?\n",
    "\n",
    "#for key, model in models.items():\n",
    "#    model.fit(X_train_scaled, y_train)\n",
    "#    print(key + \": \" + str(model.score(X_val_scaled, y_val)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
